import streamlit as st
import pandas as pd
import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from snownlp import SnowNLP
import random
import os
import platform

# ---------------------------------------------------------
# [å®‰è£èˆ‡åŸ·è¡Œæ•™å­¸]
# 1. å®‰è£å¥—ä»¶: pip install streamlit pandas matplotlib jieba wordcloud snownlp
# 2. åŸ·è¡Œç¨‹å¼: python -m streamlit run sentiment_analysis_dashboard.py
# ---------------------------------------------------------

# --- 1. ç³»çµ±é…ç½®èˆ‡å­—é«”è¨­å®š (è§£æ±ºä¸­æ–‡äº‚ç¢¼å•é¡Œ) ---
st.set_page_config(page_title="ç¤¾ç¾¤è¼¿æƒ…èˆ‡æƒ…æ„Ÿåˆ†æç³»çµ±", layout="wide")

def get_chinese_font():
    """åµæ¸¬ç³»çµ±ä¸­çš„è¨ˆç®—ä¸­æ–‡å­—é«”è·¯å¾‘ (é‡å° Windows å„ªåŒ–)"""
    system = platform.system()
    if system == "Windows":
        # å¾®è»Ÿæ­£é»‘é«”
        font_path = "C:/Windows/Fonts/msjh.ttc"
        if os.path.exists(font_path):
            return font_path
        return "C:/Windows/Fonts/simhei.ttf" # å‚™ç”¨
    elif system == "Darwin": # Mac
        return "/System/Library/Fonts/PingFang.ttc"
    return None # Linux æˆ–å…¶ä»–

CHINESE_FONT_PATH = get_chinese_font()

# è¨­å®š Matplotlib å­—é«”ä»¥é¡¯ç¤ºä¸­æ–‡
if CHINESE_FONT_PATH and os.path.exists(CHINESE_FONT_PATH):
    from matplotlib.font_manager import FontProperties
    font_prop = FontProperties(fname=CHINESE_FONT_PATH)
    plt.rcParams['font.sans-serif'] = [font_prop.get_name()]
    plt.rcParams['axes.unicode_minus'] = False
else:
    st.warning("âš ï¸ æœªåµæ¸¬åˆ°ä¸­æ–‡å­—é«”ï¼Œåœ–è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æœƒé¡¯ç¤ºç‚ºæ–¹æ¡†ã€‚")

# --- 2. æ¨¡æ“¬è³‡æ–™ç”Ÿæˆ (Mock Data) ---
def load_mock_data():
    """ç”Ÿæˆæ¨¡æ“¬çš„é¤å»³ Google Maps è©•è«–æ•¸æ“š"""
    reviews = [
        # æ­£é¢è©•è«–
        "é€™å®¶é¤å»³çš„ç‰›æ’çœŸçš„å¤ªå¥½åƒäº†ï¼Œé®®å«©å¤šæ±ï¼", "æœå‹™ç”Ÿæ…‹åº¦éå¸¸è¦ªåˆ‡ï¼Œç’°å¢ƒä¹Ÿå¾ˆä¹¾æ·¨ã€‚",
        "CPå€¼å¾ˆé«˜ï¼Œä¸‹æ¬¡ä¸€å®šæœƒå†ä¾†ã€‚", "ç”œé»æ˜¯äº®é»ï¼Œé›–ç„¶ä¸»é¤ç¨å¾®æ™®é€šï¼Œä½†æ•´é«”é«”é©—å¾ˆå¥½ã€‚",
        "å‡ºé¤é€Ÿåº¦å¿«ï¼Œå¾ˆé©åˆä¸Šç­æ—ä¸­åˆä¾†åƒã€‚", "éš±è—ç‰ˆèœå–®çœŸçš„ä»¤äººé©šè‰·ï¼",
        "é›–ç„¶åƒ¹æ ¼ç¨è²´ï¼Œä½†é£ŸæçœŸçš„å¾ˆæ–°é®®ï¼Œç‰©è¶…æ‰€å€¼ã€‚", "å®¶åº­èšé¤çš„å¥½åœ°æ–¹ï¼Œæœ‰æä¾›å…’ç«¥åº§æ¤…ã€‚",
        
        # ä¸­æ€§/æ™®é€šè©•è«–
        "å‘³é“é‚„å¯ä»¥ï¼Œä½†æ˜¯æ’éšŠæ’å¤ªä¹…äº†ã€‚", "ä¸­è¦ä¸­çŸ©ï¼Œæ²’æœ‰ç‰¹åˆ¥é©šè‰·çš„åœ°æ–¹ã€‚",
        "åƒ¹æ ¼åé«˜ï¼Œä½†ä»½é‡æœ‰é»å°‘ã€‚", "è£æ½¢å¾ˆæ¼‚äº®ï¼Œé©åˆæ‹ç…§ï¼Œä½†é£Ÿç‰©æ™®é€šã€‚",
        "åœè»Šä¸å¤ªæ–¹ä¾¿ï¼Œå»ºè­°é¨è»Šä¾†ã€‚", 
        
        # è² é¢è©•è«–
        "æœå‹™æ…‹åº¦å¾ˆå·®ï¼Œæœå‹™ç”Ÿæ„›ç†ä¸ç†çš„ã€‚", "æ¹¯é€ä¸Šä¾†æ˜¯å†·çš„ï¼Œè·Ÿåº—å®¶åæ‡‰ä¹Ÿæ²’æœ‰è™•ç†ã€‚",
        "è¡›ç”Ÿç’°å¢ƒå ªæ†‚ï¼Œæ¡Œå­é»é»çš„ã€‚", "å®Œå…¨ä¸æ¨ï¼Œé€™å€‹åƒ¹ä½å¯ä»¥åƒåˆ°æ›´å¥½çš„ã€‚",
        "ç‰›æ’ç…å¾—å¤ªè€äº†ï¼Œè·ŸçŸ³é ­ä¸€æ¨£ç¡¬ã€‚", "é ç´„äº†é‚„è¦ç­‰30åˆ†é˜ï¼Œå‹•ç·šè¦åŠƒå¾ˆäº‚ã€‚",
        "é€™æ˜¯æˆ‘åƒéæœ€ç³Ÿç³•çš„ç¾©å¤§åˆ©éºµï¼Œå¤ªé¹¹äº†ã€‚", "çµå¸³æ™‚å¤šç®—äº†éŒ¢ï¼Œè¦æ³¨æ„çœ‹å¸³å–®ã€‚"
    ]
    
    # éš¨æ©Ÿç”Ÿæˆ 50 ç­†æ•¸æ“š
    data = []
    platforms = ["Google Maps", "Facebook", "Dcard"]
    for i in range(50):
        review = random.choice(reviews)
        platform_name = random.choice(platforms)
        # ç°¡å–®æ¨¡æ“¬ï¼šå¦‚æœè©•è«–åŒ…å«è² é¢é—œéµå­—ï¼Œåˆ†æ•¸çµ¦ä½ä¸€é»
        if any(w in review for w in ["å·®", "å†·", "ç¡¬", "äº‚", "ç³Ÿç³•", "ä¸æ¨"]):
            rating = random.randint(1, 2)
        elif any(w in review for w in ["æ™®é€š", "è¿˜å¯ä»¥", "ä¹…"]):
            rating = random.randint(3, 3)
        else:
            rating = random.randint(4, 5)
            
        data.append({
            "id": i + 1,
            "platform": platform_name,
            "text": review,
            "user_rating": rating
        })
    
    return pd.DataFrame(data)

# --- 3. æ ¸å¿ƒåˆ†æåŠŸèƒ½ ---

def analyze_sentiment(df):
    """ä½¿ç”¨ SnowNLP é€²è¡Œæƒ…æ„Ÿåˆ†æ"""
    # SnowNLP çš„ sentiments å±¬æ€§æœƒå›å‚³ 0~1 çš„æ•¸å€¼ï¼Œè¶Šæ¥è¿‘ 1 ä»£è¡¨è¶Šæ­£é¢
    df['sentiment_score'] = df['text'].apply(lambda x: SnowNLP(x).sentiments)
    
    # å®šç¾©æƒ…æ„Ÿæ¨™ç±¤
    def get_label(score):
        if score > 0.6: return "æ­£é¢ (Positive)"
        elif score < 0.4: return "è² é¢ (Negative)"
        else: return "ä¸­æ€§ (Neutral)"
        
    df['sentiment_label'] = df['sentiment_score'].apply(get_label)
    return df

def generate_wordcloud(text_list):
    """ç”Ÿæˆæ–‡å­—é›²"""
    # 1. çµå·´æ–·è©
    text = " ".join(text_list)
    # è¼‰å…¥ç¹é«”çµå·´æ¨¡å¼ (å¯é¸)
    # jieba.set_dictionary('dict.txt.big') 
    
    words = jieba.cut(text)
    
    # 2. å»é™¤åœç”¨è© (Stopwords)
    stopwords = set(["çš„", "äº†", "æ˜¯", "ä¹Ÿ", "éƒ½", "å°±", "ä½†", "å¾ˆ", "åœ¨", "æœ‰", "æˆ‘", "å»", "åƒ", "é€™", "é‚£"])
    filtered_words = [w for w in words if w not in stopwords and len(w) > 1] # éæ¿¾å–®å­—
    
    final_text = " ".join(filtered_words)
    
    # 3. ç¹ªè£½æ–‡å­—é›²
    wc = WordCloud(
        font_path=CHINESE_FONT_PATH, # é‡è¦ï¼šå¿…é ˆæŒ‡å®šä¸­æ–‡å­—é«”
        background_color="white",
        width=800,
        height=400,
        max_words=100
    ).generate(final_text)
    
    return wc, filtered_words

# --- 4. Streamlit UI ä»‹é¢ ---

# å´é‚Šæ¬„
st.sidebar.title("ğŸ“Š è¼¿æƒ…åˆ†ææ§åˆ¶å°")
data_source = st.sidebar.radio("é¸æ“‡è³‡æ–™ä¾†æº", ["è¼‰å…¥ç¯„ä¾‹è³‡æ–™ (é¤å»³è©•è«–)", "ä¸Šå‚³ CSV æª”æ¡ˆ (é€²éšåŠŸèƒ½)"])

if data_source == "è¼‰å…¥ç¯„ä¾‹è³‡æ–™ (é¤å»³è©•è«–)":
    raw_df = load_mock_data()
    st.sidebar.success("âœ… ç¯„ä¾‹è³‡æ–™å·²è¼‰å…¥")
else:
    uploaded_file = st.sidebar.file_uploader("ä¸Šå‚³æ‚¨çš„è©•è«– CSV (éœ€åŒ…å« 'text' æ¬„ä½)", type="csv")
    if uploaded_file:
        raw_df = pd.read_csv(uploaded_file)
        if 'text' not in raw_df.columns:
            st.error("CSV æª”æ¡ˆå¿…é ˆåŒ…å« 'text' æ¬„ä½")
            st.stop()
    else:
        st.info("è«‹ä¸Šå‚³æª”æ¡ˆæˆ–åˆ‡æ›è‡³ç¯„ä¾‹è³‡æ–™")
        st.stop()

# ä¸»ç•«é¢
st.title("ğŸ—£ï¸ ç”¢å“/æœå‹™ è¼¿æƒ…æƒ…æ„Ÿåˆ†æå„€è¡¨æ¿")
st.markdown("é€é **NLP è‡ªç„¶èªè¨€è™•ç†** æŠ€è¡“ï¼Œè‡ªå‹•åˆ†ææ¶ˆè²»è€…è©•è«–ï¼Œæç…‰å•†æ¥­æ´å¯Ÿã€‚")

# é€²è¡Œåˆ†æ
with st.spinner('æ­£åœ¨é€²è¡Œæƒ…æ„Ÿé‹ç®—èˆ‡æ–·è©åˆ†æ...'):
    df = analyze_sentiment(raw_df)
    
# 1. æ•¸æ“šæ¦‚è¦½
col1, col2, col3 = st.columns(3)
avg_score = df['sentiment_score'].mean()
positive_ratio = (df['sentiment_label'] == 'æ­£é¢ (Positive)').mean() * 100
col1.metric("å¹³å‡æƒ…æ„Ÿåˆ†æ•¸ (0-1)", f"{avg_score:.2f}")
col2.metric("æ­£é¢è©•è«–ä½”æ¯”", f"{positive_ratio:.1f}%")
col3.metric("ç¸½è©•è«–æ•¸", f"{len(df)} å‰‡")

# 2. æƒ…æ„Ÿåˆ†ä½ˆåœ– (Pie Chart)
st.subheader("1. æƒ…æ„Ÿå‚¾å‘åˆ†ä½ˆ")
col_chart, col_table = st.columns([1, 1])

with col_chart:
    sentiment_counts = df['sentiment_label'].value_counts()
    fig1, ax1 = plt.subplots()
    ax1.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', 
            colors=['#66b3ff', '#ff9999', '#99ff99'], startangle=90)
    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    
    # å¦‚æœæ²’æœ‰ä¸­æ–‡å­—é«”ï¼Œä½¿ç”¨å‚™ç”¨é¡¯ç¤ºæ–¹å¼
    if not CHINESE_FONT_PATH:
        st.pyplot(fig1)
        st.caption("è‹¥åœ–è¡¨æ–‡å­—é¡¯ç¤ºæ–¹æ¡†ï¼Œè«‹æª¢æŸ¥ç³»çµ±æ˜¯å¦å®‰è£å¾®è»Ÿæ­£é»‘é«”")
    else:
        st.pyplot(fig1)

with col_table:
    st.dataframe(df[['text', 'sentiment_label', 'sentiment_score']].head(10), height=300)

# 3. æ–‡å­—é›²åˆ†æ
st.subheader("2. é—œéµå­—æ–‡å­—é›² (Word Cloud)")
st.markdown("åˆ†ææ¶ˆè²»è€…æœ€å¸¸æåˆ°çš„é—œéµè©ï¼š")

# åˆ†åˆ¥ç”¢ç”Ÿ æ­£é¢ vs è² é¢ æ–‡å­—é›²
sentiment_filter = st.radio("é¸æ“‡è¦åˆ†æçš„è©•è«–é¡å‹ï¼š", ["å…¨éƒ¨", "æ­£é¢ (Positive)", "è² é¢ (Negative)"], horizontal=True)

if sentiment_filter == "å…¨éƒ¨":
    target_text = df['text'].tolist()
elif sentiment_filter == "æ­£é¢ (Positive)":
    target_text = df[df['sentiment_label'] == "æ­£é¢ (Positive)"]['text'].tolist()
else:
    target_text = df[df['sentiment_label'] == "è² é¢ (Negative)"]['text'].tolist()

if target_text:
    wc_img, keywords = generate_wordcloud(target_text)
    
    # é¡¯ç¤ºæ–‡å­—é›²åœ–ç‰‡
    fig2, ax2 = plt.subplots(figsize=(10, 5))
    ax2.imshow(wc_img, interpolation='bilinear')
    ax2.axis("off")
    st.pyplot(fig2)
    
    # é¡¯ç¤ºé«˜é »è©çµ±è¨ˆ
    st.write("ğŸ”¥ **é«˜é »é—œéµè© Top 10ï¼š**")
    from collections import Counter
    word_counts = Counter(keywords).most_common(10)
    st.bar_chart(pd.DataFrame(word_counts, columns=["é—œéµå­—", "æ¬¡æ•¸"]).set_index("é—œéµå­—").T)
else:
    st.warning("æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è©•è«–è³‡æ–™ã€‚")

# 4. å•†æ¥­æ´å¯Ÿå»ºè­° (æ¨¡æ“¬ç”Ÿæˆ)
st.subheader("ğŸ’¡ å•†æ¥­æ´å¯Ÿèˆ‡å»ºè­°")
insight = ""
if avg_score < 0.4:
    insight = "âš ï¸ **è­¦ç¤ºï¼š** æ•´é«”æƒ…æ„Ÿåå‘è² é¢ã€‚æ¶ˆè²»è€…ä¸»è¦æŠ±æ€¨é›†ä¸­åœ¨ã€Œæœå‹™æ…‹åº¦ã€èˆ‡ã€Œç­‰å¾…æ™‚é–“ã€ã€‚å»ºè­°ç«‹å³æª¢è¨å¤–å ´äººå“¡åŸ¹è¨“èˆ‡è¨‚ä½æµç¨‹ã€‚"
elif avg_score > 0.7:
    insight = "âœ… **å„ªè‰¯ï¼š** å®¢æˆ¶æ»¿æ„åº¦é«˜ã€‚é—œéµå­—é¡¯ç¤ºã€Œå¥½åƒçš„ç‰›æ’ã€èˆ‡ã€ŒCPå€¼ã€æ˜¯ä¸»è¦å„ªå‹¢ï¼Œå»ºè­°åœ¨è¡ŒéŠ·ç´ æä¸­åŠ å¼·é€™äº›è³£é»ã€‚"
else:
    insight = "â„¹ï¸ **è§€å¯Ÿï¼š** è©•åƒ¹å‘ˆç¾å…©æ¥µåŒ–æˆ–æŒå¹³ã€‚éƒ¨åˆ†ç”¢å“å—åˆ°å¥½è©•ï¼Œä½†æœå‹™æµç¨‹å¯èƒ½æœ‰æ”¹å–„ç©ºé–“ã€‚éœ€é€²ä¸€æ­¥åˆ†æè² è©•ç´°ç¯€ã€‚"

st.info(insight)

# é å°¾
st.markdown("---")
st.caption("é–‹ç™¼è€…: [EddieTcLee] | æŠ€è¡“æ£§: Python, Jieba (NLP), SnowNLP, Streamlit")